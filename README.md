# AWS WAF Data Lake com Terraform

[![Terraform](https://img.shields.io/badge/Terraform-1.0+-623CE4?logo=terraform)](https://www.terraform.io/)
[![AWS](https://img.shields.io/badge/AWS-Cloud-FF9900?logo=amazonaws)](https://aws.amazon.com/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![GitHub](https://img.shields.io/github/stars/DevWizardsOps/aws-waf-data-lake?style=social)](https://github.com/DevWizardsOps/aws-waf-data-lake)

Infraestrutura completa para coletar, armazenar e analisar logs do AWS WAF usando **S3 + Glue + Athena + Grafana**, 100% provisionada via Terraform.

**üí∞ Custo total: ~$450/m√™s** (vs $4,500 Datadog ou $2,700-4,200 CloudWatch)

**üìä Reten√ß√£o: 60 dias** | **‚ö° Performance: 85% redu√ß√£o no volume escaneado**

üîó **Reposit√≥rio**: [github.com/DevWizardsOps/aws-waf-data-lake](https://github.com/DevWizardsOps/aws-waf-data-lake)

## üìã Arquitetura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   AWS WAF   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ JSON Logs
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Kinesis Firehose ‚îÇ ‚óÑ‚îÄ‚îÄ Converte JSON ‚Üí Parquet
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   S3 Bucket     ‚îÇ ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ Glue Catalog ‚îÇ
‚îÇ  (Parquet)      ‚îÇ      ‚îÇ  (Schema)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Amazon Athena  ‚îÇ ‚óÑ‚îÄ‚îÄ Queries SQL
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üèóÔ∏è Estrutura Modular

```
waf-data-lake/
‚îú‚îÄ‚îÄ main.tf                      # Orquestra todos os m√≥dulos
‚îú‚îÄ‚îÄ variables.tf                 # Vari√°veis globais
‚îú‚îÄ‚îÄ outputs.tf                   # Outputs principais
‚îú‚îÄ‚îÄ providers.tf                 # Configura√ß√£o AWS
‚îú‚îÄ‚îÄ backend.tf                   # Backend do Terraform (state)
‚îú‚îÄ‚îÄ terraform.tfvars.example     # Exemplo de configura√ß√£o
‚îî‚îÄ‚îÄ modules/
    ‚îú‚îÄ‚îÄ storage/                 # M√≥dulo S3
    ‚îÇ   ‚îú‚îÄ‚îÄ main.tf             # Bucket + Lifecycle
    ‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
    ‚îÇ   ‚îî‚îÄ‚îÄ outputs.tf
    ‚îú‚îÄ‚îÄ iam/                     # M√≥dulo IAM
    ‚îÇ   ‚îú‚îÄ‚îÄ main.tf             # Roles + Policies
    ‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
    ‚îÇ   ‚îî‚îÄ‚îÄ outputs.tf
    ‚îú‚îÄ‚îÄ glue/                    # M√≥dulo Glue
    ‚îÇ   ‚îú‚îÄ‚îÄ main.tf             # Database + Table
    ‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
    ‚îÇ   ‚îî‚îÄ‚îÄ outputs.tf
    ‚îú‚îÄ‚îÄ athena/                  # M√≥dulo Athena
    ‚îÇ   ‚îú‚îÄ‚îÄ main.tf             # Workgroup + Views
    ‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
    ‚îÇ   ‚îî‚îÄ‚îÄ outputs.tf
    ‚îú‚îÄ‚îÄ lambda/                  # M√≥dulo Lambda
    ‚îÇ   ‚îú‚îÄ‚îÄ main.tf             # Function + EventBridge
    ‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
    ‚îÇ   ‚îú‚îÄ‚îÄ outputs.tf
    ‚îÇ   ‚îî‚îÄ‚îÄ function/
    ‚îÇ       ‚îî‚îÄ‚îÄ update_views.py
    ‚îî‚îÄ‚îÄ firehose/                # M√≥dulo Firehose
        ‚îú‚îÄ‚îÄ main.tf             # Delivery Stream
        ‚îú‚îÄ‚îÄ variables.tf
        ‚îî‚îÄ‚îÄ outputs.tf
```

## üì¶ Recursos Criados

### Nomenclatura Padronizada

Todos os recursos seguem o padr√£o `waf-data-lake-*`:

| Recurso | Nome | Descri√ß√£o |
|---------|------|-----------|
| **S3 Bucket** | `waf-data-lake-logs-{account_id}-{region}` | Logs em Parquet particionados |
| **Kinesis Firehose** | `waf-data-lake-firehose` | Stream de convers√£o JSON‚ÜíParquet |
| **Glue Database** | `waf_data_lake` | Database do Data Catalog |
| **Glue Table** | `logs` | Schema dos logs WAF |
| **Athena Workgroup** | `waf-data-lake` | Workgroup customizado |
| **Athena Query Results** | `waf-data-lake-athena-results-{account_id}-{region}` | Bucket para resultados |
| **Lambda Function** | `waf-data-lake-update-views` | Fun√ß√£o de atualiza√ß√£o di√°ria |
| **EventBridge Rule** | `waf-data-lake-update-views-daily` | Agendamento di√°rio |
| **IAM Role** | `waf-data-lake-firehose-role` | Role para o Firehose |
| **IAM Policy** | `waf-data-lake-firehose-policy` | Permiss√µes S3/Glue/CloudWatch |
| **CloudWatch Log Group** | `/aws/kinesisfirehose/waf-data-lake` | Logs do Firehose |

### Configura√ß√µes

- **Particionamento S3**: `year=YYYY/month=MM/day=DD`
- **Lifecycle S3**: Apaga logs ap√≥s 30 dias (configur√°vel)
- **Buffering Firehose**: 128 MB ou 5 minutos
- **Formato**: JSON ‚Üí Parquet (compress√£o autom√°tica)
- **Timezone**: America/Sao_Paulo
- **CloudWatch Retention**: 7 dias
- **Lambda Execution**: Di√°ria √†s 2h UTC (23h Bras√≠lia)

## üöÄ Como Usar

### 1. Pr√©-requisitos

```bash
# Terraform >= 1.0
terraform --version

# AWS CLI configurado
aws configure list
```

### 2. Inicializar

```bash
cd aws-waf-data-lake
terraform init
```

### 3. Validar

```bash
terraform validate
```

### 4. Planejar (Dry-run)

```bash
terraform plan
```

Isso mostra o que ser√° criado **sem aplicar** nenhuma mudan√ßa.

### 5. Aplicar

```bash
terraform apply
```

Digite `yes` para confirmar.

### 6. Ver Outputs

```bash
terraform output
terraform output data_lake_summary
```

### 7. Obter Credenciais do Grafana

Ap√≥s o `terraform apply`, obtenha as credenciais do usu√°rio IAM para Grafana:

```bash
# Ver Access Key ID
terraform output grafana_access_key_id

# Ver Secret Access Key (use -raw para copiar facilmente)
terraform output -raw grafana_secret_access_key

# Ver configura√ß√£o completa do datasource
terraform output grafana_configuration
```

**‚ö†Ô∏è Importante:**
- As credenciais s√£o armazenadas no `terraform.tfstate` (arquivo sens√≠vel)
- Nunca commite o `terraform.tfstate` no Git
- Para produ√ß√£o, considere usar AWS Secrets Manager

## üìä Configura√ß√£o do Grafana

### Adicionar Datasource Athena

1. **Acesse:** Grafana ‚Üí Configuration ‚Üí Data Sources ‚Üí Add data source
2. **Busque:** "Amazon Athena"
3. **Configure:**
   - **Name**: `WAF Data Lake`
   - **Authentication Provider**: `Access & secret key`
   - **Access Key ID**: (do output `grafana_access_key_id`)
   - **Secret Access Key**: (do output `grafana_secret_access_key`)
   - **Default Region**: `sa-east-1`
   - **Workgroup**: `waf-data-lake`
   - **Database**: `waf_data_lake`
   - **Output Location**: `s3://waf-data-lake-athena-results-<ACCOUNT_ID>-sa-east-1/`

4. **Teste a conex√£o** clicando em "Save & Test"

### Importar Dashboards

Os dashboards prontos est√£o em [`grafana/`](grafana/):

**Dashboards Dispon√≠veis:**
- **waf-logs-explorer.json** - Explora√ß√£o interativa de logs com filtros din√¢micos
- **waf-overview.json** - Vis√£o executiva de seguran√ßa
- **waf-views-optimized.json** - Dashboard otimizado usando views pr√©-calculadas (performance superior)
- **waf-block-investigation.json** - Investiga√ß√£o detalhada de bloqueios com filtro por origin/host

**Recursos dos Dashboards:**
- ‚úÖ Filtros por IP, Pa√≠s, Regra, Origin (Host)
- ‚úÖ Timeline de bloqueios em tempo real
- ‚úÖ Top IPs, pa√≠ses e regras bloqueadas
- ‚úÖ An√°lise por m√©todo HTTP e c√≥digo de resposta
- ‚úÖ Logs detalhados com link para IPInfo
- ‚úÖ Campo "origin" (host) para identificar qual aplica√ß√£o est√° sob ataque

**Via Interface (UI):**
```bash
# 1. Acesse: Grafana ‚Üí Dashboards ‚Üí Import
# 2. Clique em "Upload JSON file"
# 3. Selecione um dos arquivos da pasta grafana/
```

**Via API (Autom√°tico):**
```bash
# WAF Logs Explorer
curl -X POST http://localhost:3000/api/dashboards/db \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_GRAFANA_API_KEY" \
  -d @grafana/waf-logs-explorer.json

# WAF Block Investigation (Otimizado)
curl -X POST http://localhost:3000/api/dashboards/db \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_GRAFANA_API_KEY" \
  -d @grafana/waf-views-optimized.json
```

Veja documenta√ß√£o completa em [`grafana.md`](grafana.md).

## üîß Customiza√ß√£o

### Op√ß√£o 1: Editar variables.tf

Altere os valores padr√£o em [variables.tf](variables.tf).

### Op√ß√£o 2: Criar terraform.tfvars

```bash
cp terraform.tfvars.example terraform.tfvars
```

Edite `terraform.tfvars`:

```hcl
project_name       = "waf-data-lake"
aws_region         = "sa-east-1"
aws_profile        = "default"  # ou seu profile AWS CLI
log_retention_days = 60

tags = {
  Environment = "production"
  Team        = "Security"
}
```

## üìä Consultar Logs com Athena

### Views Pr√©-configuradas

O m√≥dulo Athena cria automaticamente 10 views otimizadas para an√°lise:

1. **vw_daily_summary** - Resumo di√°rio de requisi√ß√µes
2. **vw_top_blocked_ips** - IPs mais bloqueados
3. **vw_requests_by_country** - Estat√≠sticas por pa√≠s
4. **vw_rule_performance** - Performance das regras WAF
5. **vw_http_method_analysis** - An√°lise por m√©todo HTTP
6. **vw_response_codes** - Distribui√ß√£o de c√≥digos HTTP
7. **vw_blocks_timeline** - Timeline de bloqueios por regra (otimizada com filtro de 7 dias)
8. **vw_block_investigation** - Logs detalhados para investiga√ß√£o com campo origin
9. **vw_blocks_by_rule_type** - Bloqueios agrupados por tipo de regra
10. **vw_top_blocked_rules** - Top regras que mais bloqueiam

**‚ö° Otimiza√ß√£o de Performance**: Views de investiga√ß√£o filtram automaticamente √∫ltimos 7 dias, reduzindo volume escaneado de ~12TB para ~1.4TB (85% de redu√ß√£o).

**ü§ñ Atualiza√ß√£o Autom√°tica**: Uma fun√ß√£o Lambda executa diariamente √†s 2h UTC (23h Bras√≠lia) para recriar todas as views com os dados mais recentes.

### Usar as Views

```sql
-- Ver resumo di√°rio
SELECT * FROM waf_data_lake.vw_daily_summary
WHERE year = '2026' AND month = '01'
LIMIT 100;

-- Top 20 IPs bloqueados
SELECT * FROM waf_data_lake.vw_top_blocked_ips
LIMIT 20;

-- An√°lise por pa√≠s
SELECT * FROM waf_data_lake.vw_requests_by_country
ORDER BY blocked DESC;
```

### Query b√°sica na tabela principal

```sql
SELECT 
  FROM_UNIXTIME(timestamp/1000) as request_time,
  httprequest.clientip as client_ip,
  httprequest.country as country,
  httprequest.uri as uri,
  httprequest.httpmethod as method,
  action,
  responsecodesent
FROM waf_data_lake.logs
WHERE year = '2026'
  AND month = '01'
  AND day = '09'
LIMIT 100;
```

### Top IPs bloqueados

```sql
SELECT 
  httprequest.clientip as ip,
  httprequest.country as country,
  COUNT(*) as total_blocks
FROM waf_data_lake.logs
WHERE action = 'BLOCK'
  AND year = '2026'
  AND month = '01'
GROUP BY httprequest.clientip, httprequest.country
ORDER BY total_blocks DESC
LIMIT 20;
```

### An√°lise por regra

```sql
SELECT 
  terminatingruleid,
  action,
  COUNT(*) as total
FROM waf_data_lake.logs
WHERE year = '2026'
  AND month = '01'
GROUP BY terminatingruleid, action
ORDER BY total DESC;
```

## üîÑ Importar Recursos Existentes

Se voc√™ j√° tem recursos criados manualmente e quer gerenci√°-los com Terraform:

```bash
# S3 Bucket
terraform import module.storage.aws_s3_bucket.logs waf-logs-parquet-<ACCOUNT_ID>-<REGION>

# Glue Database
terraform import module.glue.aws_glue_catalog_database.this waf_logs

# Glue Table
terraform import module.glue.aws_glue_catalog_table.waf_logs waf_logs:waf_logs_schema

# IAM Role
terraform import module.iam.aws_iam_role.firehose KinesisFirehoseServiceRole-aws-waf-logs--sa-east-1-1763344426520

# Kinesis Firehose
terraform import module.firehose.aws_kinesis_firehose_delivery_stream.this aws-waf-logs-to-s3-parquet

# CloudWatch Log Group
terraform import aws_cloudwatch_log_group.firehose /aws/kinesisfirehose/aws-waf-logs-to-s3-parquet
```

‚ö†Ô∏è **Nota**: Ap√≥s importar, ajuste as vari√°veis para corresponder aos recursos existentes.

## üóÇÔ∏è M√≥dulos

### Storage (S3)

- Bucket com nomenclatura padronizada
- Lifecycle policy configur√°vel
- Public access bloqueado por padr√£o
- Tags customiz√°veis

**Localiza√ß√£o**: [modules/storage/](modules/storage/)

### IAM

- Role para Firehose com assume policy
- Policy com permiss√µes m√≠nimas (S3, Glue, CloudWatch)
- Attachment autom√°tico

**Localiza√ß√£o**: [modules/iam/](modules/iam/)

### Glue

- Database do Data Catalog
- Tabela com schema completo dos logs WAF
- Suporte a location customizado

**Localiza√ß√£o**: [modules/glue/](modules/glue/)

### Athena

- Workgroup customizado com configura√ß√µes
- Bucket S3 para query results com lifecycle
- 6 views pr√©-configuradas para an√°lise
- Named queries para facilitar consultas

**Localiza√ß√£o**: [modules/athena/](modules/athena/)

### Lambda

- Fun√ß√£o Python 3.12 para atualizar views
- Execu√ß√£o autom√°tica di√°ria via EventBridge
- Logs detalhados no CloudWatch
- Permiss√µes IAM para Athena, Glue e S3

**Localiza√ß√£o**: [modules/lambda/](modules/lambda/)

### Firehose

- Delivery stream com convers√£o JSON‚ÜíParquet
- Buffering configur√°vel
- Particionamento por data
- Logging no CloudWatch

**Localiza√ß√£o**: [modules/firehose/](modules/firehose/)

## üîê Backend Remoto (Opcional)

Para trabalho em equipe, configure o backend S3 em [backend.tf](backend.tf):

```hcl
terraform {
  backend "s3" {
    bucket         = "your-terraform-state-bucket"
    key            = "waf-data-lake/terraform.tfstate"
    region         = "sa-east-1"
    profile        = "default"  # ou seu profile AWS CLI
    encrypt        = true
    dynamodb_table = "terraform-state-lock"
  }
}
```

## üßπ Limpeza

Para destruir todos os recursos:

```bash
terraform destroy
```

‚ö†Ô∏è **CUIDADO**: Isso apagar√°:
- Bucket S3 (incluindo logs)
- Firehose stream
- Glue database e table
- IAM roles e policies
- CloudWatch log groups

## üìù Vari√°veis Dispon√≠veis

| Vari√°vel | Descri√ß√£o | Padr√£o |
|----------|-----------|--------|
| `project_name` | Nome do projeto | `waf-data-lake` |
| `aws_region` | Regi√£o AWS | `sa-east-1` |
| `aws_profile` | Profile AWS CLI | `default` |
| `account_id` | ID da conta AWS | `<ACCOUNT_ID>` |
| `log_retention_days` | Reten√ß√£o logs S3 | `60` |
| `glue_database_name` | Nome database Glue | `waf_data_lake` |
| `glue_table_name` | Nome tabela Glue | `logs` |
| `cloudwatch_log_retention_days` | Reten√ß√£o CloudWatch | `7` |
| `athena_query_results_retention_days` | Reten√ß√£o resultados Athena | `7` |
| `lambda_schedule_expression` | Agendamento Lambda | `cron(0 2 * * ? *)` |

Veja todas em [variables.tf](variables.tf).

## üÜò Troubleshooting

### Erro: Bucket j√° existe
```
Error: creating Amazon S3 Bucket (waf-data-lake-logs-...)
```
**Solu√ß√£o**: Use `terraform import` ou mude o nome do bucket em `variables.tf`.

### Erro: Glue table j√° existe
```
Error: creating Glue Catalog Table
```
**Solu√ß√£o**: Importe a tabela existente ou use outro nome.

### Erro: IAM role j√° existe
```
Error: creating IAM Role
```
**Solu√ß√£o**: Importe a role ou ajuste o nome em `modules/iam/main.tf`.

## üìö Documenta√ß√£o

- [AWS Kinesis Firehose](https://docs.aws.amazon.com/firehose/)
- [AWS Glue Data Catalog](https://docs.aws.amazon.com/glue/)
- [Amazon Athena](https://docs.aws.amazon.com/athena/)
- [Terraform AWS Provider](https://registry.terraform.io/providers/hashicorp/aws/)

## üí° Artigos e Documenta√ß√£o

- üìù [Artigo Completo no LinkedIn](docs/ARTIGO_LINKEDIN.md) - Hist√≥ria completa da jornada de otimiza√ß√£o
- üìù [Artigo Conciso](docs/ARTIGO_LINKEDIN_CONCISO.md) - Vers√£o resumida para compartilhamento

## ü§ù Contribuindo

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudan√ßas (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## üìÑ Licen√ßa

Distribu√≠do sob a licen√ßa MIT. Veja `LICENSE` para mais informa√ß√µes.

## üìû Contato

DevWizardsOps - [@DevWizardsOps](https://github.com/DevWizardsOps)

Link do Projeto: [https://github.com/DevWizardsOps/aws-waf-data-lake](https://github.com/DevWizardsOps/aws-waf-data-lake)

---

‚≠ê Se este projeto foi √∫til, considere dar uma estrela no GitHub!
